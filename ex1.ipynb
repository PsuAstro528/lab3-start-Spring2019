{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Astro 528, Lab 3, Exercise 1\n",
    "\n",
    "# Benchmarking File I/O & Calling Python \n",
    "\n",
    "For many applications, its important that we be able to read input data from a file and/or to write our outputes to files so they can be reused later.  Disk access is typically much slower than accessing system memory.  Therefore, disk access can easily become the limiting factor for a project.  In this set of exercises, you'll see examples of how to perform basic file I/O.  \n",
    "\n",
    "You'll be provided with most of the code you need, so that you can focus on comparing how much disk space and time is required by different file formats.  Near the end of the lab, you'll be asked to think about when each type of file format would be a good choice for you to use in your research projects.\n",
    "\n",
    "First, let's make sure you have all the packages you'll need installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:22:09.675000-05:00",
     "start_time": "2019-01-22T06:22:03.895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[?25l\u001b[2K\u001b[?25h"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\".\")\n",
    "Pkg.instantiate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download some data\n",
    "First, we're going to download some data from the web.  Julia has a built in `download` function that can be handy for this.  It relies on your system having some utilities already installed (e.g., `curl`, `wget` or `fetch`).  If you run into trouble, then you can leave the cell below (for testing purposes), but manually download the file to the data subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:53:01.105000-05:00",
     "start_time": "2019-01-22T06:53:01.097Z"
    }
   },
   "outputs": [],
   "source": [
    "path = basename(pwd())==\"test\" ? \"../data/\" : \"data/\"\n",
    "url = \"https://exoplanetarchive.ipac.caltech.edu/data/KeplerData/Simulated/kplr_dr25_inj1_plti.txt\"\n",
    "filename_ipac = joinpath(path,basename(url))    # extract the filename and prepend \"data/\"\n",
    "if !isfile(filename_ipac)\n",
    "    println(\"# Will now attempt to download \" * url * \" into \" * filename_ipac)\n",
    "    download(url,filename_ipac)           # download the file\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, everything you needed for an assignment was included in a GitHub repository.  So why did I make you download the file?\n",
    "\n",
    "Notice the size of the file.  Git is great for tracking source code, but it wasn't really designed for working with large files (especially large _binary_ files).  Since  we're not going to be editing it, we'll simply download it once.  Besides, it's useful to know how to download a file from within a julia script.  \n",
    "\n",
    "### Install packages to storage, read and write data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I've picked a file containing the results of applying the pipeline for NASA's Kepler mission to [simulated data](https://exoplanetarchive.ipac.caltech.edu/docs/KeplerSimulated.html) in which the signals of simulated \"planet's\" have been injected into actual Kepler data.  This data set is the basis for computing the efficiency of the Kepler pipeline at detecting real planets.  This dataset has played an important role in enabling astronomers to estimate the occurrence rates of planets around other stars.  \n",
    "For documentation of its contents, you could read [its documentation](https://exoplanetarchive.ipac.caltech.edu/docs/KSCI-19110-001.pdf).  However, for now, we'll just do some basic manipulations of the file, so the details of its contents aren't important.  That said, it is important to know the _file format_.  \n",
    "\n",
    "This data file that we downloaded is in [IPAC format](https://irsa.ipac.caltech.edu/applications/DDGEN/Doc/ipac_tbl.html).  \n",
    "It would be tedious to learn the details of every file format that is used in astronomy, let alone to write our own code to read them.  Fortunately, there are packages that can read the most common file formats.  \n",
    "This an example of something that [astropy](http://docs.astropy.org) is particularly good for.  It provides a function [`astropy.io.ascii.read`](http://docs.astropy.org/en/stable/io/ascii/) that will read a file in IPAC for us.  \n",
    "Since astropy is written in Python, we import the [`PyCall`](https://github.com/stevengj/PyCall.jl) package, so we can import python packages and call python functions from Julia.  \n",
    "Since reading a file from disk is typically limited by the rate of getting data from disk, rather than compute speed, it's usually not a problem that Python isn't particularly fast, when it's comes to reading files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:22:11.241000-05:00",
     "start_time": "2019-01-22T06:22:03.902Z"
    }
   },
   "outputs": [],
   "source": [
    "using PyCall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to import the required python module using the `@pyimport` macro.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:22:13.786000-05:00",
     "start_time": "2019-01-22T06:22:03.904Z"
    }
   },
   "outputs": [],
   "source": [
    "@pyimport astropy.io.ascii as astropy_io_ascii  # Import from SciPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can review the documentation for [`astropy.io.ascii.read`](http://docs.astropy.org/en/stable/io/ascii/) and call that function load the data in our input file.  Let's use `@time`, so we can compare the time required to read various formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:22:18.665000-05:00",
     "start_time": "2019-01-22T06:22:03.907Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4.159844 seconds (829.46 k allocations: 42.006 MiB, 0.26% gc time)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<i>Table masked=True length=146294</i>\n",
       "<table id=\"table139692875634336\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>KIC_ID</th><th>Sky_Group</th><th>i_period</th><th>i_epoch</th><th>N_Transit</th><th>i_depth</th><th>i_dur</th><th>i_b</th><th>i_ror</th><th>i_dor</th><th>EB_injection</th><th>Offset_from_source</th><th>Offset_distance</th><th>Expected_MES</th><th>Recovered</th><th>TCE_ID</th><th>Measured_MES</th><th>r_period</th><th>r_epoch</th><th>r_depth</th><th>r_dur</th><th>r_b</th><th>r_ror</th><th>r_dor</th><th>Fit_Provenance</th></tr></thead>\n",
       "<thead><tr><th></th><th></th><th>(days)</th><th>(BKJD)</th><th></th><th>(ppm)</th><th>(hrs)</th><th></th><th></th><th></th><th></th><th></th><th>arcsec</th><th></th><th></th><th></th><th></th><th>(days)</th><th>(BKJD)</th><th>(ppm)</th><th>(hrs)</th><th></th><th></th><th></th><th></th></tr></thead>\n",
       "<thead><tr><th>int64</th><th>int64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>int64</th><th>int64</th><th>float64</th><th>float64</th><th>int64</th><th>str12</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>int64</th></tr></thead>\n",
       "<tr><td>11445144</td><td>8</td><td>70.0385</td><td>107.4886</td><td>0.9808</td><td>1662.0</td><td>4.0203</td><td>0.4141</td><td>0.0306</td><td>126.498</td><td>0</td><td>0</td><td>0.0</td><td>2.4719</td><td>0</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "<tr><td>11496840</td><td>8</td><td>87.8829</td><td>103.9753</td><td>0.9808</td><td>1561.0</td><td>2.20705</td><td>0.7649</td><td>0.035</td><td>214.397</td><td>0</td><td>0</td><td>0.0</td><td>0.9453</td><td>0</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "<tr><td>11497236</td><td>8</td><td>88.302</td><td>84.4552</td><td>0.9808</td><td>1332.0</td><td>2.11234</td><td>0.81</td><td>0.0334</td><td>207.336</td><td>0</td><td>0</td><td>0.0</td><td>0.9322</td><td>0</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "<tr><td>11548779</td><td>8</td><td>35.4942</td><td>73.9562</td><td>2.9425</td><td>686.0</td><td>1.00655</td><td>0.8915</td><td>0.0247</td><td>138.661</td><td>0</td><td>0</td><td>0.0</td><td>1.1577</td><td>0</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "<tr><td>11600744</td><td>8</td><td>48.8212</td><td>97.2396</td><td>23.2943</td><td>372.0</td><td>1.77915</td><td>0.8738</td><td>0.0183</td><td>110.768</td><td>0</td><td>0</td><td>0.0</td><td>3.9829</td><td>0</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "<tr><td>11601794</td><td>8</td><td>71.3789</td><td>111.5236</td><td>15.1325</td><td>838.0</td><td>3.88664</td><td>0.3761</td><td>0.0232</td><td>133.934</td><td>0</td><td>0</td><td>0.0</td><td>8.7399</td><td>1</td><td>011601794-01</td><td>8.4883</td><td>71.3804</td><td>182.8861</td><td>710.0</td><td>4.07025</td><td>0.499</td><td>0.025</td><td>119.969</td><td>1</td></tr>\n",
       "<tr><td>11651634</td><td>8</td><td>12.199</td><td>65.4292</td><td>93.9127</td><td>417.0</td><td>1.92268</td><td>0.3893</td><td>0.0144</td><td>45.626</td><td>0</td><td>0</td><td>0.0</td><td>6.5484</td><td>0</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "<tr><td>11652051</td><td>8</td><td>19.905</td><td>68.6416</td><td>56.0697</td><td>481.0</td><td>2.76096</td><td>0.0016</td><td>0.0159</td><td>56.138</td><td>0</td><td>0</td><td>0.0</td><td>6.7866</td><td>0</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "<tr><td>11652704</td><td>8</td><td>10.0683</td><td>71.4388</td><td>8.8276</td><td>685.0</td><td>1.75635</td><td>0.3136</td><td>0.0206</td><td>42.672</td><td>0</td><td>0</td><td>0.0</td><td>1.5078</td><td>0</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "<tr><td>11652732</td><td>8</td><td>53.4462</td><td>118.0207</td><td>6.2772</td><td>761.0</td><td>2.35004</td><td>0.4757</td><td>0.0232</td><td>157.822</td><td>0</td><td>0</td><td>0.0</td><td>4.2792</td><td>0</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "<tr><td>11653019</td><td>8</td><td>99.054</td><td>92.7997</td><td>2.2068</td><td>1543.0</td><td>2.00733</td><td>0.8637</td><td>0.0369</td><td>220.191</td><td>0</td><td>0</td><td>0.0</td><td>2.8227</td><td>0</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "<tr><td>11653212</td><td>8</td><td>26.1243</td><td>80.7006</td><td>3.9234</td><td>1710.0</td><td>3.16547</td><td>0.2079</td><td>0.0278</td><td>64.021</td><td>0</td><td>0</td><td>0.0</td><td>2.4685</td><td>0</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "<tr><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr>\n",
       "<tr><td>10412138</td><td>39</td><td>291.8522</td><td>144.9414</td><td>3.0514</td><td>991.0</td><td>8.75231</td><td>0.5098</td><td>0.0257</td><td>227.804</td><td>0</td><td>0</td><td>0.0</td><td>7.8638</td><td>1</td><td>010412138-01</td><td>7.7716</td><td>291.8575</td><td>144.9216</td><td>831.0</td><td>8.48282</td><td>0.112</td><td>0.026</td><td>268.064</td><td>1</td></tr>\n",
       "<tr><td>10412175</td><td>39</td><td>151.4665</td><td>79.3649</td><td>2.9751</td><td>422.0</td><td>86.91703</td><td>0.6799</td><td>0.019</td><td>10.154</td><td>0</td><td>0</td><td>0.0</td><td>1.4771</td><td>0</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "<tr><td>10412223</td><td>39</td><td>313.1902</td><td>326.5634</td><td>4.9858</td><td>394.0</td><td>6.72894</td><td>0.7704</td><td>0.0186</td><td>237.695</td><td>0</td><td>0</td><td>0.0</td><td>5.0399</td><td>0</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "<tr><td>10412226</td><td>39</td><td>144.6225</td><td>207.3081</td><td>7.8465</td><td>614.0</td><td>9.53371</td><td>0.2943</td><td>0.0214</td><td>113.505</td><td>0</td><td>0</td><td>0.0</td><td>9.3471</td><td>1</td><td>010412226-01</td><td>7.6857</td><td>144.6259</td><td>207.2914</td><td>438.0</td><td>9.40849</td><td>0.701</td><td>0.0205</td><td>87.092</td><td>1</td></tr>\n",
       "<tr><td>10477502</td><td>39</td><td>293.2682</td><td>263.8007</td><td>2.0084</td><td>80.0</td><td>10.6595</td><td>0.4241</td><td>0.0082</td><td>192.252</td><td>0</td><td>0</td><td>0.0</td><td>4.69</td><td>0</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "<tr><td>10477662</td><td>39</td><td>294.5506</td><td>311.439</td><td>4.9041</td><td>299.0</td><td>27.81212</td><td>0.0252</td><td>0.015</td><td>82.139</td><td>0</td><td>0</td><td>0.0</td><td>15.4175</td><td>1</td><td>010477662-01</td><td>10.4672</td><td>294.5511</td><td>311.6225</td><td>224.0</td><td>18.12219</td><td>0.496</td><td>0.014</td><td>109.811</td><td>1</td></tr>\n",
       "<tr><td>10477697</td><td>39</td><td>203.8994</td><td>221.6245</td><td>6.081</td><td>83.0</td><td>2.29948</td><td>0.9871</td><td>0.011</td><td>150.367</td><td>0</td><td>0</td><td>0.0</td><td>2.1017</td><td>0</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "<tr><td>10477707</td><td>39</td><td>174.7915</td><td>87.6557</td><td>7.9555</td><td>323.0</td><td>8.9704</td><td>0.517</td><td>0.0158</td><td>130.349</td><td>0</td><td>0</td><td>0.0</td><td>12.8021</td><td>1</td><td>010477707-01</td><td>12.493</td><td>174.7944</td><td>262.4339</td><td>285.0</td><td>8.94583</td><td>0.703</td><td>0.0166</td><td>109.656</td><td>1</td></tr>\n",
       "<tr><td>10477733</td><td>39</td><td>278.8801</td><td>278.958</td><td>5.0348</td><td>283.0</td><td>24.00014</td><td>0.436</td><td>0.0151</td><td>81.416</td><td>0</td><td>0</td><td>0.0</td><td>6.8601</td><td>0</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "<tr><td>10477762</td><td>39</td><td>246.9499</td><td>231.9186</td><td>5.0022</td><td>962.0</td><td>85.25417</td><td>0.8561</td><td>0.0312</td><td>12.823</td><td>0</td><td>0</td><td>0.0</td><td>6.6907</td><td>0</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "<tr><td>10477794</td><td>39</td><td>15.3987</td><td>75.575</td><td>92.0984</td><td>148.0</td><td>5.08524</td><td>0.1119</td><td>0.0101</td><td>23.253</td><td>0</td><td>0</td><td>0.0</td><td>6.4567</td><td>0</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "<tr><td>10477878</td><td>39</td><td>44.5426</td><td>81.7792</td><td>26.622</td><td>414.0</td><td>3.26506</td><td>0.8828</td><td>0.02</td><td>53.65</td><td>0</td><td>0</td><td>0.0</td><td>7.9734</td><td>0</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td><td>--</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "PyObject <Table masked=True length=146294>\n",
       " KIC_ID  Sky_Group i_period i_epoch  ...   r_b    r_ror   r_dor  Fit_Provenance\n",
       "                    (days)   (BKJD)  ...                                       \n",
       " int64     int64   float64  float64  ... float64 float64 float64     int64     \n",
       "-------- --------- -------- -------- ... ------- ------- ------- --------------\n",
       "11445144         8  70.0385 107.4886 ...      --      --      --             --\n",
       "11496840         8  87.8829 103.9753 ...      --      --      --             --\n",
       "11497236         8   88.302  84.4552 ...      --      --      --             --\n",
       "11548779         8  35.4942  73.9562 ...      --      --      --             --\n",
       "11600744         8  48.8212  97.2396 ...      --      --      --             --\n",
       "11601794         8  71.3789 111.5236 ...   0.499   0.025 119.969              1\n",
       "11651634         8   12.199  65.4292 ...      --      --      --             --\n",
       "11652051         8   19.905  68.6416 ...      --      --      --             --\n",
       "11652704         8  10.0683  71.4388 ...      --      --      --             --\n",
       "11652732         8  53.4462 118.0207 ...      --      --      --             --\n",
       "11653019         8   99.054  92.7997 ...      --      --      --             --\n",
       "11653212         8  26.1243  80.7006 ...      --      --      --             --\n",
       "     ...       ...      ...      ... ...     ...     ...     ...            ...\n",
       "10412138        39 291.8522 144.9414 ...   0.112   0.026 268.064              1\n",
       "10412175        39 151.4665  79.3649 ...      --      --      --             --\n",
       "10412223        39 313.1902 326.5634 ...      --      --      --             --\n",
       "10412226        39 144.6225 207.3081 ...   0.701  0.0205  87.092              1\n",
       "10477502        39 293.2682 263.8007 ...      --      --      --             --\n",
       "10477662        39 294.5506  311.439 ...   0.496   0.014 109.811              1\n",
       "10477697        39 203.8994 221.6245 ...      --      --      --             --\n",
       "10477707        39 174.7915  87.6557 ...   0.703  0.0166 109.656              1\n",
       "10477733        39 278.8801  278.958 ...      --      --      --             --\n",
       "10477762        39 246.9499 231.9186 ...      --      --      --             --\n",
       "10477794        39  15.3987   75.575 ...      --      --      --             --\n",
       "10477878        39  44.5426  81.7792 ...      --      --      --             --"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time data_from_astropy = astropy_io_ascii.read(filename_ipac, format=\"ipac\", fast_reader=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T18:11:55.155000-05:00",
     "start_time": "2019-01-20T23:11:53.882Z"
    }
   },
   "source": [
    "On the plus side, the data was read in.  However, what is the type of the data we just read and stored into data_from_astropy?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:22:18.994000-05:00",
     "start_time": "2019-01-22T06:22:03.909Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typeof(data_from_astropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T18:11:55.155000-05:00",
     "start_time": "2019-01-20T23:11:53.882Z"
    }
   },
   "source": [
    "Since Python is weakly-typed language, it's type is `PyObject`.  That can contain most anything!  \n",
    "That flexibility can be convenient, but it is also one of the reasons that Python is not a good language for high-performance computing.  To enable Julia to work efficiently with the data, we'll want Julia to know what type the data is and store a list of strictly-typed columns into a the data into a `DataFrame`.\n",
    "The PyCall package provides an interface for For accessing data from PyObjects.  Because of the weak typing issue, the syntax is a bit funny.  \n",
    "\n",
    "If you're not intending to call Python for your class project, then there's no reason to worry about that these details.  So this list is just for students who are curious about accessing Python data and methods from Julia.\n",
    "- Given `o::PyObject`, `o[:attribute]` is equivalent to `o.attribute` in Python, with automatic type conversion.  \n",
    "- Given `o::PyObject`, `get(o, key)` is equivalent to `o[key]` in Python, with automatic type conversion.  \n",
    "- There's more information about accessing data in PyObjects (and other types to contain Python data) in the [PyCall documentation](https://github.com/JuliaPy/PyCall.jl#types).\n",
    "\n",
    "For now, you can get a [\"Dictionary\"](https://docs.julialang.org/en/v1/base/collections/index.html#Dictionaries-1) by using `data_from_astropy[:columns]`.  The dictionary consists of a set of _keys_ (in this case strings), where each key is associated with a _value_ (in this case a Vector or 1-d array).  \n",
    "Often times, the data you want to work with can be represented as a table.  For efficiency's sake, it's usually best to represent these as a bunch of `Vector`'s, each containing one columns of data.  Using a `Dict` allows you to give the columns names (instead of just numbers) and allows each column to have a different type (again useful for Julia to optimize your code).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:22:20.188000-05:00",
     "start_time": "2019-01-22T06:22:03.913Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.140132 seconds (275.77 k allocations: 40.853 MiB, 12.69% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{Any,Any} with 25 entries:\n",
       "  \"i_b\"                => [0.4141, 0.7649, 0.81, 0.8915, 0.8738, 0.3761, 0.3893…\n",
       "  \"r_period\"           => [0.0, 0.0, 0.0, 0.0, 0.0, 71.3804, 0.0, 0.0, 0.0, 0.0…\n",
       "  \"r_ror\"              => [0.0, 0.0, 0.0, 0.0, 0.0, 0.025, 0.0, 0.0, 0.0, 0.0  …\n",
       "  \"TCE_ID\"             => PyObject <MaskedColumn name='TCE_ID' dtype='str12' le…\n",
       "  \"r_b\"                => [0.0, 0.0, 0.0, 0.0, 0.0, 0.499, 0.0, 0.0, 0.0, 0.0  …\n",
       "  \"i_dor\"              => [126.498, 214.397, 207.336, 138.661, 110.768, 133.934…\n",
       "  \"Measured_MES\"       => [0.0, 0.0, 0.0, 0.0, 0.0, 8.4883, 0.0, 0.0, 0.0, 0.0 …\n",
       "  \"Offset_from_source\" => [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0…\n",
       "  \"i_period\"           => [70.0385, 87.8829, 88.302, 35.4942, 48.8212, 71.3789,…\n",
       "  \"Fit_Provenance\"     => [0, 0, 0, 0, 0, 1, 0, 0, 0, 0  …  0, 1, 0, 1, 0, 1, 0…\n",
       "  \"i_depth\"            => [1662.0, 1561.0, 1332.0, 686.0, 372.0, 838.0, 417.0, …\n",
       "  \"N_Transit\"          => [0.9808, 0.9808, 0.9808, 2.9425, 23.2943, 15.1325, 93…\n",
       "  \"r_epoch\"            => [0.0, 0.0, 0.0, 0.0, 0.0, 182.886, 0.0, 0.0, 0.0, 0.0…\n",
       "  \"Expected_MES\"       => [2.4719, 0.9453, 0.9322, 1.1577, 3.9829, 8.7399, 6.54…\n",
       "  \"i_dur\"              => [4.0203, 2.20705, 2.11234, 1.00655, 1.77915, 3.88664,…\n",
       "  \"i_ror\"              => [0.0306, 0.035, 0.0334, 0.0247, 0.0183, 0.0232, 0.014…\n",
       "  \"Sky_Group\"          => [8, 8, 8, 8, 8, 8, 8, 8, 8, 8  …  39, 39, 39, 39, 39,…\n",
       "  \"Recovered\"          => [0, 0, 0, 0, 0, 1, 0, 0, 0, 0  …  0, 1, 0, 1, 0, 1, 0…\n",
       "  \"r_dor\"              => [0.0, 0.0, 0.0, 0.0, 0.0, 119.969, 0.0, 0.0, 0.0, 0.0…\n",
       "  \"KIC_ID\"             => [11445144, 11496840, 11497236, 11548779, 11600744, 11…\n",
       "  \"Offset_distance\"    => [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  … …\n",
       "  \"i_epoch\"            => [107.489, 103.975, 84.4552, 73.9562, 97.2396, 111.524…\n",
       "  \"r_depth\"            => [0.0, 0.0, 0.0, 0.0, 0.0, 710.0, 0.0, 0.0, 0.0, 0.0  …\n",
       "  \"EB_injection\"       => [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0…\n",
       "  \"r_dur\"              => [0.0, 0.0, 0.0, 0.0, 0.0, 4.07025, 0.0, 0.0, 0.0, 0.0…"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time dict = data_from_astropy[:columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T18:11:55.155000-05:00",
     "start_time": "2019-01-20T23:11:53.882Z"
    }
   },
   "source": [
    "Most of the columns can be automatically converted to an array with a known Julia type.  For example, `data_from_astropy[:columns][\"KIC_ID\"]` returns data as an `Array{Int64,1}`.  However, value associated with `TCE_ID` is some `PyObject` that can't be automatically reinterpretted as a Julia array.  Since many KIC_IDs don't have an associated TCE_ID, there are many missing entries.  Python is trying to store a list with lots of empty entries efficiently, but PyCall doesn't (yet?) know how to deal with this \"masked array\".  Working with the data when Julia can't know its type would be very inefficient.  Therefore, we want to create an array of Strings that allows Julia to represent this data more efficiently.  Technically, it will be an array where each element is either a `String` or a `missing`.  It took a little tinkering, but eventually, I figured out how to extract that data into an efficient Julia object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:22:21.570000-05:00",
     "start_time": "2019-01-22T06:22:03.915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.611832 seconds (1.21 M allocations: 70.965 MiB, 4.30% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "146294-element Array{Union{Missing, String},1}:\n",
       " missing       \n",
       " missing       \n",
       " missing       \n",
       " missing       \n",
       " missing       \n",
       " \"011601794-01\"\n",
       " missing       \n",
       " missing       \n",
       " missing       \n",
       " missing       \n",
       " missing       \n",
       " missing       \n",
       " missing       \n",
       " ⋮             \n",
       " \"010412138-01\"\n",
       " missing       \n",
       " missing       \n",
       " \"010412226-01\"\n",
       " missing       \n",
       " \"010477662-01\"\n",
       " missing       \n",
       " \"010477707-01\"\n",
       " missing       \n",
       " missing       \n",
       " missing       \n",
       " missing       "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time TCE_ID_list = map(x -> x != nothing ? x : missing, data_from_astropy[:columns][\"TCE_ID\"][\"data\"][\"tolist\"]())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T20:37:57.272000-05:00",
     "start_time": "2019-01-21T01:37:57.074Z"
    }
   },
   "source": [
    "Now let's replace the value associated with TCE_ID with this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:22:21.640000-05:00",
     "start_time": "2019-01-22T06:22:03.919Z"
    }
   },
   "outputs": [],
   "source": [
    "dict[\"TCE_ID\"] = TCE_ID_list;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just wantted to access the data, then we could use the data stored in dict as a dictionary.  \n",
    "However, a dictionary doesn't guarentee anything about the relationship of the value of different keys.\n",
    "For example, in a table, each column should have the same number of rows.  Therefore, we'll switch from representing the data as a dictionary and start using a `DataFrame`. A `DataFrame` can be thought of as a table, where the data for each column is stored as an array.   A `DataFrame` also provides some additional features to allow easy and efficient access and manipulation of the table that will come in useful later.  \n",
    "\n",
    "First, we'll import the [DataFrames.jl package](https://github.com/JuliaData/DataFrames.jl) and create a small `DataFrame` (so the functions get compiled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:22:32.121000-05:00",
     "start_time": "2019-01-22T06:22:03.921Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>a</th><th>b</th><th>c</th></tr><tr><th></th><th>Int64</th><th>Int64</th><th>String⍰</th></tr></thead><tbody><p>2 rows × 3 columns</p><tr><th>1</th><td>1</td><td>3</td><td>hello</td></tr><tr><th>2</th><td>2</td><td>4</td><td>missing</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccc}\n",
       "\t& a & b & c\\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Int64 & String⍰\\\\\n",
       "\t\\hline\n",
       "\t1 & 1 & 3 & hello \\\\\n",
       "\t2 & 2 & 4 &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "2×3 DataFrame\n",
       "│ Row │ a     │ b     │ c       │\n",
       "│     │ \u001b[90mInt64\u001b[39m │ \u001b[90mInt64\u001b[39m │ \u001b[90mString⍰\u001b[39m │\n",
       "├─────┼───────┼───────┼─────────┤\n",
       "│ 1   │ 1     │ 3     │ hello   │\n",
       "│ 2   │ 2     │ 4     │ \u001b[90mmissing\u001b[39m │"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using DataFrames\n",
    "# Create a small DataFrame so compilation time won't be included below\n",
    "small_df = DataFrame(a=[1,2],b=[3,4],c=[\"hello\",missing])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:22:34.042000-05:00",
     "start_time": "2019-01-22T06:22:03.924Z"
    }
   },
   "source": [
    "Then, we'll use our existing dictionary to initialize a `DataFrame`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:22:34.042000-05:00",
     "start_time": "2019-01-22T06:22:03.924Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.085065 seconds (153.31 k allocations: 7.663 MiB, 9.72% gc time)\n"
     ]
    }
   ],
   "source": [
    "@time df = DataFrame(dict); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T23:32:18.744Z"
    }
   },
   "source": [
    "Look at how much memory was allocated during this line of code.  Did Julia make a new copy of all of the data?  Any of the data?\n",
    "\n",
    "INSERT RESPONSE\n",
    "\n",
    "\n",
    "## Writing a CSV file\n",
    "\n",
    "The IPAC format allows for significant metadata, but reading it can be annoying.  \n",
    "Let's say that we'd like to write the data to a [CSV file](https://en.wikipedia.org/wiki/Comma-separated_values), so that it's easier for other programs to read in.  We could import the CSV package and write it out with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:22:43.053000-05:00",
     "start_time": "2019-01-22T06:22:03.927Z"
    }
   },
   "outputs": [],
   "source": [
    "using CSV\n",
    "# Read & write a small test file so compilation time not included below\n",
    "small_df = DataFrame(a=[1,2],b=[3,4],c=[\"hello\",missing])\n",
    "CSV.write(joinpath(path,\"silly.csv\"),small_df)  \n",
    "CSV.read(joinpath(path,\"silly.csv\")); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:22:44.953000-05:00",
     "start_time": "2019-01-22T06:22:03.929Z"
    }
   },
   "source": [
    "Now, let's write our DataFrame to a CSV file and time how long it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:22:44.953000-05:00",
     "start_time": "2019-01-22T06:22:03.929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.259962 seconds (9.75 M allocations: 242.672 MiB, 3.38% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"data/kplr_dr25_inj1_plti.csv\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename_csv = replace(filename_ipac, \".txt\" => \".csv\") \n",
    "@time CSV.write(filename_csv,df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing time to read text files\n",
    "\n",
    "Look back at how long it took to read in the file in IPAC format.  How long do you think it will take to read in the same data in CSV format?\n",
    "    \n",
    "INSERT RESPONCE\n",
    "    \n",
    "Now, try reading it in and see.   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:55:11.537000-05:00",
     "start_time": "2019-01-22T06:55:10.870Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.655737 seconds (7.22 M allocations: 165.147 MiB, 18.40% gc time)\n"
     ]
    }
   ],
   "source": [
    "@time df_csv = CSV.read(filename_csv);   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did the time required to read the data in CSV format compare to the time to read the data in IPAC format?\n",
    "\n",
    "Next, we'll compare the filesizes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:22:46.289000-05:00",
     "start_time": "2019-01-22T06:22:03.934Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47257124"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filesize(filename_ipac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:22:46.291000-05:00",
     "start_time": "2019-01-22T06:22:03.937Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19860247"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filesize(filename_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the sizes of the files in the two formats compare?  \n",
    "\n",
    "INSERT RESPONSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T00:21:21.943000-05:00",
     "start_time": "2019-01-21T05:21:21.935Z"
    }
   },
   "source": [
    "## Cleaning up memory\n",
    "\n",
    "If we were writing a proper program with nearly all of the work occurring inside functions, then Julia's garbage collector could recognize when data is no longer accessible automatically.  However, since we're working in a Jupyter notebook, the variables that we set outside of functions are of global scope.  Julia doesn't know which we intend to use again and which we're done with.  \n",
    "For this exercise, we're working with some largish datasets, so it may be helpful to free up memory when it's not longer need.  \n",
    "Here's an example of a macro that will do that for us.[^1]\n",
    "\n",
    "[^1]: Note that Python's garbage collector is separate from Julia's.  I'm not sure if the part about Python is doing what I want.  If there are any python experts, I'd appreciate feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:22:47.312000-05:00",
     "start_time": "2019-01-22T06:22:03.941Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "@purge_data"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@pyimport gc as pygc                 # Import Python's garbage collector (need to avoid conflicts)\n",
    "\n",
    "\"Set var to nothing and explicitly call the garbage collector.\"\n",
    "macro purge_data(var)\n",
    "    return :( \n",
    "        call_python_gc = (typeof($var) <: PyObject) ? true : false;\n",
    "        global $var = nothing;       # Set variable to nothing\n",
    "        GC.gc();                     # Call Julia's garbage collector\n",
    "        if call_python_gc \n",
    "            # println(\"# Calling Python's garbage collector\")\n",
    "            pygc.collect()          # Call Python's garbage collector\n",
    "        end;\n",
    "        nothing;\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this macro to purge data in variables that we won't be needing anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:22:47.758000-05:00",
     "start_time": "2019-01-22T06:22:03.944Z"
    }
   },
   "outputs": [],
   "source": [
    "@purge_data data_from_astropy\n",
    "@purge_data dict\n",
    "@purge_data df_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T00:30:37.585000-05:00",
     "start_time": "2019-01-21T05:30:37.577Z"
    }
   },
   "source": [
    "Now, the variables still exist in our namespace, but we try to access one of those variables, we will get `nothing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:22:47.763000-05:00",
     "start_time": "2019-01-22T06:22:03.947Z"
    }
   },
   "outputs": [],
   "source": [
    "data_from_astropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing time to read binary files\n",
    "\n",
    "There are numerous binary file formats that one could use.  Here, we'll try using JLD2 which is a subset of the [HDF5](https://www.hdfgroup.org/solutions/hdf5/) file format.  This means that when [Julia's JLD2 package](https://github.com/JuliaIO/JLD2.jl) writes jld2 files, they can be read by other programs that can read HDF5 files.  However, a generic HDF5 file is not a valid JLD2 file.  If you want to read a HDF5 file, then you can use Julia's [HDF5.jl package](https://github.com/JuliaIO/HDF5.jl).  The [FileIO.jl](https://github.com/JuliaIO/FileIO.jl) package provides a common interface for reading and writing from multiple file formats.\n",
    "\n",
    "As before, we'll load the packages and call each function once using a small DataFrame, just so they get compiled before we benchmark them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:23:03.057000-05:00",
     "start_time": "2019-01-22T06:22:03.949Z"
    }
   },
   "outputs": [],
   "source": [
    "using JLD2, FileIO\n",
    "@save joinpath(path,\"silly.jld2\") small_df \n",
    "@load joinpath(path,\"silly.jld2\") small_df\n",
    "load(joinpath(path,\"silly.jld2\"), \"small_df\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:23:03.433000-05:00",
     "start_time": "2019-01-22T06:22:03.952Z"
    }
   },
   "source": [
    "Now time how long it takes to save the data to a JLD2 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:23:03.433000-05:00",
     "start_time": "2019-01-22T06:22:03.952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.234330 seconds (350.46 k allocations: 22.235 MiB, 4.74% gc time)\n"
     ]
    }
   ],
   "source": [
    "filename_jld2 = replace(filename_ipac, \".txt\" => \".jld2\") \n",
    "@time @save filename_jld2 df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now time how long it takes to loead the data from the JLD2 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:23:03.674000-05:00",
     "start_time": "2019-01-22T06:22:03.955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.196767 seconds (844.72 k allocations: 65.955 MiB, 10.23% gc time)\n"
     ]
    }
   ],
   "source": [
    "@time df_jld2 = load(filename_jld2,\"df\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll compare the filesizes for the JLD2 file to the CSV file.  How large would you guess the JLD2 file will be?\n",
    "\n",
    "INSERT RESPONSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:23:03.676000-05:00",
     "start_time": "2019-01-22T06:22:03.958Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36600791"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filesize(filename_jld2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:05:15.428000-05:00",
     "start_time": "2019-01-21T03:05:15.407Z"
    }
   },
   "source": [
    "How did the actual JLD2 filesize compare to the CSV filesize?  \n",
    "What explains this difference?  Can you think of a circumstance when the relative sizes would switch?\n",
    "\n",
    "INSERT RESPONSE\n",
    "\n",
    "We won't need the `df_jld2` DataFrame again, so let's purge it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:23:03.767000-05:00",
     "start_time": "2019-01-22T06:22:03.961Z"
    }
   },
   "outputs": [],
   "source": [
    "@purge_data df_jld2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:17:19.481000-05:00",
     "start_time": "2019-01-21T03:17:19.470Z"
    }
   },
   "source": [
    "## Feather\n",
    "\n",
    "[Feather](https://github.com/wesm/feather) is another binary file format that is based on [Apache Arrow](https://arrow.apache.org/) and that aims to provide efficient storage, and fast reading and writing.  It pays particular attention to strings of variable length (very useful if dealing with webpages, tweets, etc.).  There is a native Julia implementation, [Feather.jl](http://juliadata.github.io/Feather.jl/stable/) that we can try.  As before, we'll import the package and execute each command once before timing the reading and writing of our big data table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:23:07.571000-05:00",
     "start_time": "2019-01-22T06:22:03.963Z"
    }
   },
   "outputs": [],
   "source": [
    "using Feather\n",
    "filename_feather = replace(filename_ipac, \".txt\" => \".feather\") \n",
    "Feather.write(filename_feather, small_df)\n",
    "Feather.read(filename_feather);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:23:08.381000-05:00",
     "start_time": "2019-01-22T06:22:03.966Z"
    }
   },
   "source": [
    "Now, let's time how long it takes to write and read our dataframe as feather files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:23:08.381000-05:00",
     "start_time": "2019-01-22T06:22:03.966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.300330 seconds (648.02 k allocations: 257.830 MiB, 10.16% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"data/kplr_dr25_inj1_plti.feather\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time Feather.write(filename_feather, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:23:08.666000-05:00",
     "start_time": "2019-01-22T06:22:03.969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.265885 seconds (534.12 k allocations: 27.061 MiB, 2.37% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29238392"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time df_feather = Feather.read(filename_feather)\n",
    "filesize(filename_feather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:17:19.481000-05:00",
     "start_time": "2019-01-21T03:17:19.470Z"
    }
   },
   "source": [
    "How did the runtime and size of the feather compare to those the JLD2 file?  \n",
    "\n",
    "INSERT RESPONSE\n",
    "\n",
    "Now we can purge the data in `df_feather`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:23:08.757000-05:00",
     "start_time": "2019-01-22T06:22:03.971Z"
    }
   },
   "outputs": [],
   "source": [
    "@purge_data df_feather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:17:19.481000-05:00",
     "start_time": "2019-01-21T03:17:19.470Z"
    }
   },
   "source": [
    "## FITS\n",
    "\n",
    "Astronomers often use the [FITS file format](https://en.wikipedia.org/wiki/FITS).  Like [HDF5](https://www.hdfgroup.org/solutions/hdf5/), it's a very flexible and thus complicated file format.  \n",
    "Therefore, most languages call a common [FITSIO library written in C](https://heasarc.gsfc.nasa.gov/fitsio/), rather than implementing code themselves.  Indeed, that's what [Julia's FITSIO.jl package](https://github.com/JuliaAstro/FITSIO.jl) does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:23:08.803000-05:00",
     "start_time": "2019-01-22T06:22:03.975Z"
    }
   },
   "outputs": [],
   "source": [
    "using FITSIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:13:24.628000-05:00",
     "start_time": "2019-01-21T03:13:24.603Z"
    }
   },
   "source": [
    "Unfortunately, the FITSIO package isn't as polished as the others.  It expects a `Dict` rather than a `DataFrame`, and it can't handle missing values.  So we'll make some helper functions.  Also, FITS files have complicated headers, so I'll provide a function to read all the tabular data from a simple FITS file.  Then we'll use each function once, so that Julia compiles them before we start timing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:23:11.027000-05:00",
     "start_time": "2019-01-22T06:22:03.978Z"
    }
   },
   "outputs": [],
   "source": [
    "\"Convert a DataFrame to a Dict, replacing missing values with 0 or an empty string.\"\n",
    "function convert_dataframe_to_dict_remove_missing(df::DataFrame)\n",
    "    d = Dict(map(n->\"$n\"=>               # create a dictionary\n",
    "              ( any(ismissing.(df[n])) ? # if column contains a missing\n",
    "                    map(x-> !ismissing(x) ? # search for missings\n",
    "                        x :                 # leave non-missing values alone\n",
    "                        ( (eltype(df[n]) <: Number) ? zero(eltype(n)) : \"\")\n",
    "                        , df[n])            # but replace missing with 0 or \"\"\n",
    "                : df[n] ), # if nothing is missing, just use column as is\n",
    "            names(df) ))  \n",
    "end\n",
    "\n",
    "\"Write a DataFrame to a FITS file, replacing missing values with 0 or an empty string.\"\n",
    "function write_dataframe_as_fits(filename::String, df::DataFrame)\n",
    "    try \n",
    "       dict = convert_dataframe_to_dict_remove_missing(df) \n",
    "       fits_file = FITS(filename,\"w\")\n",
    "       write(fits_file, dict )\n",
    "       close(fits_file)\n",
    "    catch\n",
    "        @warn(\"There was a problem writing a dataframe to \" * filename * \".\")\n",
    "    end\n",
    "end\n",
    "\n",
    "\"Read the columns of the first table from a FITS file into a Dict\"\n",
    "function read_fits_tables(filename::String)\n",
    "    dict = Dict{String,Any}()\n",
    "    fits_file = FITS(filename,\"r\")\n",
    "    # fits_file[1] is image data, we're interested in the table\n",
    "    @assert length(fits_file) >= 2\n",
    "    header = read_header(fits_file[2])\n",
    "    for i in 1:length(header)\n",
    "        c = get_comment(header,i)\n",
    "        if !occursin(\"label for field\",c)\n",
    "            continue\n",
    "        end\n",
    "        h = header[i]\n",
    "        @assert typeof(h) == String\n",
    "        try  \n",
    "            dict[h] = read(fits_file[2],h)\n",
    "        catch\n",
    "            @warn \"# Problem reading table column \" * h * \".\"\n",
    "        end\n",
    "    end\n",
    "    close(fits_file)\n",
    "    return dict\n",
    "end\n",
    "\n",
    "write_dataframe_as_fits(joinpath(path,\"silly.fits\"),small_df)\n",
    "read_fits_tables(joinpath(path,\"silly.fits\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can time how long it takes to write and read the data as FITS files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:23:12.394000-05:00",
     "start_time": "2019-01-22T06:22:03.981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.364873 seconds (349.58 k allocations: 21.530 MiB, 0.44% gc time)\n"
     ]
    }
   ],
   "source": [
    "filename_fits = replace(filename_ipac, \".txt\" => \".fits\") \n",
    "@time write_dataframe_as_fits(filename_fits,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:23:12.746000-05:00",
     "start_time": "2019-01-22T06:22:03.983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.330713 seconds (452.46 k allocations: 61.458 MiB, 5.72% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29856960"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time read_fits_tables(filename_fits);\n",
    "filesize(filename_fits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T00:07:15.214000-05:00",
     "start_time": "2019-01-21T05:07:14.891Z"
    }
   },
   "source": [
    "## Making a small/faster file\n",
    "\n",
    "Imagine that you were working with a table was even bigger than this one and wanted to make it smaller and load even faster.  \n",
    "Do you have any ideas how you could store the data more efficiently?\n",
    "\n",
    "INSERT RESPONSE\n",
    "\n",
    "The function below attempts to create a more efficient representation of this same data.\n",
    "Then we write it to disk as a feather file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:23:14.494000-05:00",
     "start_time": "2019-01-22T06:22:03.986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.710527 seconds (5.73 M allocations: 511.090 MiB, 10.05% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"data/kplr_dr25_inj1_plti_small.feather\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function compact_my_data(df::DataFrame)\n",
    "   df_new = Dict{Symbol,Any}(n => nothing for n in names(df))\n",
    "   # Use TCE_ID as is\n",
    "   df_new[:TCE_ID] = df[:TCE_ID]\n",
    "   # Convert KIC_ID into a 4 byte integer\n",
    "   df_new[:KIC_ID] = convert.(Int32,df[:KIC_ID])\n",
    "   # Convert Recovered and Sky_Group into a 1 byte integer\n",
    "   map(k-> df_new[k] = convert.(Int8,df[k]), [:Recovered,:Sky_Group])\n",
    "   # Convert next set of variables into 1 bit each\n",
    "   map(k-> df_new[k] = convert.(Bool,df[k]), [:EB_injection, :Fit_Provenance, :Offset_from_source])\n",
    "   # Convert next set of variables into 4 byte floats\n",
    "   map(k-> df_new[k] = convert.(Float32,df[k]), [:N_Transit,:i_b,:i_ror,:i_dur,:i_dor,:i_epoch,:i_period,:i_depth,:Expected_MES,:Measured_MES])\n",
    "   # Store Offset_distance as an array with missing values  if Offset_from_source==0\n",
    "   df_new[:Offset_distance] = convert(Array{Union{Float32,Missing},1}, df[:Offset_distance])\n",
    "   df_new[:Offset_distance][.!df_new[:Offset_from_source]] .= missing\n",
    "   # Store r_* columns arrays with missing values if Recovered == 0\n",
    "   map(k-> (df_new[k] = convert(Array{Union{Float32,Missing},1}, df[k]);\n",
    "         df_new[k][df_new[:Recovered].==0] .= missing ),\n",
    "               [:r_epoch,:r_dor,:r_depth,:r_ror,:r_b,:r_dur,:r_period])\n",
    "   DataFrame(df_new)\n",
    "end\n",
    "\n",
    "filename_small_feather = replace(filename_feather, \".feather\" => \"_small.feather\") \n",
    "@time Feather.write(filename_small_feather, compact_my_data(df) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T01:23:14.898000-05:00",
     "start_time": "2019-01-22T06:22:03.989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.401349 seconds (706.16 k allocations: 35.976 MiB, 2.31% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12762136"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time df_feather = Feather.read(filename_small_feather)\n",
    "filesize(filename_small_feather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the filesize and read speed compare to the original feather file?  Can you explain why?\n",
    "\n",
    "INSERT RESPONSE\n",
    "\n",
    "Try to provide one example when it would make sense to use each of the following file formats:\n",
    "- CSV\n",
    "- JLD2\n",
    "- Feather\n",
    "- FITS\n",
    "\n",
    "INSERT RESPONSE\n",
    "\n",
    "If you've made plans for your class project, then what file format(s) are you planning on using?  Why?\n",
    "\n",
    "INSERT RESPONSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.2",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
